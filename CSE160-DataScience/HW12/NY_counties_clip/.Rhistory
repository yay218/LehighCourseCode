data(iris)
m <- dim(iris)[1] # returns the size of the first dimension
m
val <- sample(1:m, size=round(m/3), replace=FALSE, prob = rep(1/m, m))
iris.train <- iris[-val,]; iris.test <- iris[val,]
dim(iris.train)
dim(iris.test)
library(kknn)
iris.kknn <- kknn(Species ~ ., iris.train, iris.test, distance = 2)
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.test$Species, fit)
pcol <- as.character(as.numeric(iris.test$Species))
pairs(iris.test[1:4], pch=pcol, col=c("green3", "red")[(iris.test$Species != fit)+1])
head(iris.test$Species)
#head(fit)
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
print_clusters(groups) # shows the contents of each cluster
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
print_clusters(groups) # shows the contents of each cluster
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
print_clusters(groups) # shows the contents of each cluster
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters(groups) # shows the contents of each cluster
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
library(hclust)
library(cutree)
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
print_clusters(groups) # shows the contents of each cluster
library(hclust)
library(cutree)
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Scorecard-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
print_clusters(groups) # shows the contents of each cluster
library(hclust)
library(cutree)
college <- read.csv("~/Downloads/Most-Recent-Cohorts-Treasury-Elements.csv", na.strings=c("PrivacySuppressed", "NULL"))
# the original data had multiple types of invalid entries that we need to convert to NAs
dim(college) # just to see how much data we have
college$COUNT_ED <- NULL # this column was all NA
c1 <- na.omit(college) # eliminates all rows containing NAs
c2 <- c1[1:50,] # just to look at a small subsets
d <- dist(c2[,5:88]) #calculate the distance between all points in c2, based only on columns 5-88
clusters <- hclust(d, method="ward.D") # perform hierarchical clustering
plot(clusters, labels=c2$INSTNM) # show the dendogram
groups <- cutree(clusters, k=4) # cut the dendogram at 4 clusters and record which items belong in which cluster
# Below is a function to print the contents of each cluster (using just a few attributes)
print_clusters <- function(labels) {
for (i in 1:max(labels)) {
print (paste("cluster", i))
print(c2[labels==i, c("INSTNM", "MEDIAN_HH_INC", "PCT_BORN_US")])
}
}
print_clusters(groups) # shows the contents of each cluster
#install.packages("kknn")
library(kknn)
train<-read.csv("http://www.cse.lehigh.edu/~brian/course/2017/datascience/TitanicTrain.csv", sep = ",")
test<-read.csv("http://www.cse.lehigh.edu/~brian/course/2017/datascience/TitanicTest.csv", sep = ",")
train$age <- round(train$age, digits = 0)
train$fare <- round(train$fare, digits = 2)
test$age <- round(test$age, digits = 0)
test$fare <- round(test$fare, digits = 2)
glm_model <- glm(formula = as.factor(survived) ~ pclass + fare + sex + age + embarked, data=train, family=binomial)
glm_prediction <- predict(glm_model, test, type = 'response')
glm_table <- table(glm_prediction > 0.5, test$survived)
glm_table
glm_TP <- glm_table[1,1]
glm_FP <- glm_table[1,2]
glm_FN <- glm_table[2,1]
glm_TN <- glm_table[2,2]
glm_precision <- glm_TP / (glm_TP + glm_FP)
glm_recall <- glm_TP / (glm_TP + glm_FN)
glm_fmeasure <- 2*(glm_precision * glm_recall) / (glm_precision + glm_recall)
glm_accuracy <- sum(diag(glm_table)) / sum(glm_table)
sprintf("The precision for glm model is: %s", glm_precision)
sprintf("The recall for glm model is: %s", glm_recall)
sprintf("The f measure for glm model is: %s", glm_fmeasure)
sprintf("The accuracy for glm model is: %s", glm_accuracy)
kknn_model <- kknn(survived ~ pclass + sex + age + fare + embarked, train = train, test = test, distance = 1)
fit <- fitted(kknn_model)
kknn_table <- table(fit > 0.5, test$survived)
kknn_table
kknn_TP <- kknn_table[1,1]
kknn_FP <- kknn_table[1,2]
kknn_FN <- kknn_table[2,1]
kknn_TN <- kknn_table[2,2]
kknn_precision <- kknn_TP / (kknn_TP + kknn_FP)
kknn_recall <- kknn_TP / (kknn_TP + kknn_FN)
kknn_fmeasure <- 2 * (kknn_precision * kknn_recall) / (kknn_precision + kknn_recall)
kknn_accuracy <- sum(diag(kknn_table)) / sum(kknn_table)
sprintf("The precision for knn model is: %s", kknn_precision)
sprintf("The recall for knn model is: %s", kknn_recall)
sprintf("The f measure for knn model is: %s", kknn_fmeasure)
sprintf("The accuracy for knn model is: %s", kknn_accuracy)
#Yang Yi
#11/08/2017
#HW10
#Load and install packages and library
#install.packages("e1071")
library(e1071)
#Read data and set column names
cancer_data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", sep=",", header=F, col.names = c("Sample_code_number", "Clump Thickness", "Uniformity of Cell Size", "Uniformity of Cell Shape", "Marginal Adhesion", "Single Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class"),na.strings = c("NA"))
#Clean data
cancer_data$Sample_code_number <- NULL
cancer_data$Class <- as.factor(cancer_data$Class)
#Create 10 folds
cancer_data<-cancer_data[sample(nrow(cancer_data)),]
folds <- cut(seq(1,nrow(cancer_data)),breaks=10,labels=FALSE)
#Initialize total values
total_precision = 0
total_recall = 0
total_fmeasure = 0
total_accuracy = 0
#Perform 10 fold cross validation
for (i in 1:10) {
index<-which(folds==i,arr.ind=T)
train <- cancer_data[-index,]
test <- cancer_data[index,]
i= i + 1
m <- naiveBayes(train[, -10], train[, 10])
#Create confusion matrix and tp, fp, tn, fn
table <- table(predict(m, test[, -10]), test[, 10])
TP <- table[4]
FP <- table[2]
TN <- table[1]
FN <- table[3]
#Calculate accuracy
precision <- (TP / (TP + FP))
recall <- (TP / (TP + FN))
fmeasure <- (2 * precision * recall)/(precision + recall)
accuracy <- (TP + TN) / (sum(table))
total_precision = total_precision + precision
total_recall = total_recall + recall
total_fmeasure = total_fmeasure + fmeasure
total_accuracy = total_accuracy + accuracy
}
#Print values
cat("For 10 fold cross validation:",
"\nThe average Precision is", total_precision/10,
"\nThe Average Recall is", total_recall/10,
"\nThe Average Fmeasure is", total_fmeasure/10,
"\nThe Average Accuracy is", total_accuracy/10)
#Yang Yi
#12/05/2017
#HW12
library(maptools)
library(ggplot2)
library(ggmap)
# read administrative boundaries
setwd("/Users/yiyang/Desktop/NY_counties_clip")
myMap <- readShapePoly(fn="NY_counties_clip.shp")
# extract sequence of points and assign IDs to states
myMapDf <- fortify(myMap)
library(foreign) # needed for read.dbf()
db <- read.dbf("NY_counties_clip.dbf")
db$rn <- as.integer(rownames(db))
db
# ggplot mapping, limited to area around USA
# data layer (empty map)
m0 <- ggplot(data=myMapDf) + xlim(-81,-72) + ylim(40,46)
# empty map + borders
m1 <- m0 + geom_path(aes(x=long, y=lat, group=group), color='gray')
m1 + geom_polygon(aes(x=long, y=lat, group=group))
myMapDf$id3 <- as.integer(myMapDf$id)
m0 <- ggplot(data=myMapDf) + xlim(-81,-72)+ylim(40,46)
m1 <- m0 + geom_path(aes(x=long, y=lat, group=group), color='gray')
m2 <- m1 + geom_polygon(aes(x=long, y=lat, group=group, fill=id3))
#m2
m0 <- ggplot(data=myMapDf) + xlim(-81,-72) + ylim(40,46)
m1 <- m0 + geom_polygon(aes(x=long, y=lat, group=group, fill=id3))
m2 <- m1 + geom_path(aes(x=long, y=lat, group=group), color='black')
#m2
map <- get_map(location = 'New York State', zoom=5)
m0 <- ggmap(map) + xlim(-81,-72) + ylim(40,46)
m1 <- m0 + geom_polygon(aes(x=long, y=lat, group=group, fill=id3), data=myMapDf,alpha=.7)
m2 <- m1 + geom_path(aes(x=long, y=lat, group=group), data=myMapDf, color='black')
#m2
income <- read.csv("/Users/yiyang/Desktop/nyincome2.csv", header=TRUE)
db2 <- merge(db, income, by.x="NAME", by.y="County")
db2$Median_household_income<-as.numeric(db2$Median_household_income)
myMapAugDf <- merge(myMapDf, db2, by.x="id3", by.y="rn")
map <- get_map(location = 'New York State', zoom=5)
m0 <- ggmap(map) + xlim(-81,-72) + ylim(40,46)
m1 <- m0 + geom_polygon(aes(x=long, y=lat, group=group, fill=Median_household_income),data=myMapAugDf, alpha=.7) + scale_fill_gradient(low="red", high="blue")
m2 <- m1 + geom_path(aes(x=long, y=lat, group=group), data=myMapDf,color='black')
#m2
library(doBy)
txtVal <- summaryBy(long + lat ~ id3, data=myMapDf, FUN=mean, keep.names=T)
m3 <- m2 + geom_text(aes(x=long, y=lat, label=db[id3+1,"NAME"]), data=txtVal, col="black", cex=2)
m3 + ggtitle("Median NY County Incomes") + xlab("longitude") + ylab("latitude")
m3
m3 + ggtitle("Median NY County Incomes") + xlab("longitude") + ylab("latitude")
